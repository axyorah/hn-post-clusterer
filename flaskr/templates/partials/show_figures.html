<ul class="nav nav-tabs" id="figures-tab" role="tablist">
    
    <li class="nav-item" role="presentation">
        <button class="nav-link active" id="same-label-tab-btn" data-bs-toggle="tab" 
            data-bs-target="#counts-bar-tab" aria-selected="true">
            <h2>Post Count</h2>
        </button>
    </li>

    <li class="nav-item" role="presentation">
        <button class="nav-link" id="same-label-tab-btn" data-bs-toggle="tab" 
            data-bs-target="#embed-scatter-tab" aria-selected="false">
            <h2>2D Embeddings</h2>
        </button>
    </li>

    <li class="nav-item" role="presentation">
        <button class="nav-link" id="same-label-tab-btn" data-bs-toggle="tab" 
            data-bs-target="#wordcloud-tab" aria-selected="false">
            <h2>Word Clouds</h2>
        </button>
    </li>

</ul>

<div class="tab-content" id="tab-content">

    <section class="tab-pane fade show active" id="counts-bar-tab" role="tabpanel" aria-labelledby="counts-bar-tab">
        <div class="box">
            <p>Amount of posts in each cluster.</p>
            <div id="counts-bar-plot-root"></div>
        </div>
    </section>

    <section class="tab-pane fade" id="embed-scatter-tab" role="tabpanel" aria-labelledby="embed-scatter-tab">
        <div class="box">
            <p>
                Post embeddings projected to 2D plane. 
            </p>

            <div id="embed-pca-plot-root"></div>

            <p>
                Recall, that embedding for each post is calculated by collecting all comments 
                that belong to given post, calculating all the individual sentence embeddings 
                for these comments and averaging the result.
            </p>
            <p>
                DistilRoBERTa transformer that we used here produces 
                <code>768</code>-dimensional embeddings vectors, which is a bit tricky to visualize.
                Figure above shows 2D projection of all the post embeddings 
                that was obtained with PCA. 
                Was some information about the original data lost while creating a 2D projection? 
                Oh, absolutely! Figure below shows how much variance in the original data 
                is explained with first PCA vectors. <code>TODO: explain plot</code>

            </p>

            <div id="pca-explained-variance-root"></div>

            <p>
                Linear dimensionality-reduction methods like PCA are simple and quick,
                but the results might not always look satisfactory. 
                If you want clusters to be more, well, clustered, 
                you can try some nonlinear methods, such as t-SNE. 
            </p>
            <p>
                The idea behind t-SNE is to distort the original space 
                so that closer data points get even closer together 
                and distant data points get even further apart. 
                What is "close" and what is "distant", though? 
                You can regulate what should be considered close or distant 
                by varying the <code>perplexity</code> parameter.
                You can very loosely interpret it as a number of 
                effective neighbours for each point. 
                If you set perplexity to be a lower number 
                the algorithm would favour smaller clusters; 
                if you choose it to be high the algorithm will converge 
                with a few larger clusters. 
                It is recommended to choose perplexity within <code>5 - 50</code> range.
            </p>
            <p>
                Another paramer you can consider is the number of 
                first PCA vectors that will be used as inputs to t-SNE. 
                t-SNE is quite computationally taxing, so it might be a good idea 
                to feed to t-SNE reduced-dimensionality PCA projections 
                instead of the original <code>786</code>-dimensional DistilRoBERTa embeddings. 
                Choose first <code>5 - 50</code> PCA vectors. 
                You might find the above graph quite useful.
            </p>
            <p>
                To  run t-SNE just hit the <code class="mark">Prettify</code> button below. 
            </p>

            {% include "partials/set_tsne_filters.html" %}
            
            <div id="embed-tsne-plot-root"></div>
        </div>
    </section>

    <section class="tab-pane fade" id="wordcloud-tab" role="tabpanel" aria-labelledby="wordcloud-tab">
        <div class="box">
            <p>
                While you can expect that the posts clustered together 
                would be semantically similar, it might still be a bit problematic 
                to label clusters with common "human" tags, 
                like like "cybersecurity", "mathematics" or "health".  
            </p>
            <p>
                Your best attempts at giving clusters names might look like  
                "posts about security issues and license disputes" or 
                "posts about medicine in general and COVID in particular with occasional politics". 
                The "quality" of your clusters 
                (how similar the posts clustered together are from a "human" perspective) 
                would depend among others on the number of clusters you've chosen and 
                suitability of the embedding model for the data that are clustered.
            </p>
            <p>
                One of the simpler ways of getting the idea 
                of what each cluster is about involves generating <code>Word Clouds</code>. 
                The idea behind the Word Cloud is very simple: 
                the more frequent the term appears in the text - 
                the larger it looks on a visual cloud. 
                (As a reminder: text in our case corresponts to 
                all <code>comments</code> of the posts clustered together)
            </p>
            <p>
                Cluster Word Clouds should make it easier for you 
                to navigate in the deluge of posts. 
                However, the processing steps that are needed 
                to generate Word Clouds might take some time. 
                Hit the <code class="mark">Generate</code> button 
                and go make some tea - it will take a while :)
            </p>

            <p id="wordcloud-info"></p>

            <button id="wordcloud-btn">Generate WordClouds</button>

            <div id="wordcloud-plot-root"></div>
        </div>
    </section>

</div>